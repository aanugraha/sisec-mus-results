method_name:        LSTM_NN
short:              UHL2
authors:            Stefan Uhlich, Marcello Porcu, Franck Giron, Michael Enenkl, Thomas Kemp, Yuki Mitsufuji, Naoya Takahashi
affiliation:        Sony Corporation
email:              stefan.uhlich@eu.sony.com, yuhki.mitsufuji@jp.sony.com
is_supervised:      True
code:
uses_augmentation:  True
references:
  - S. Uhlich, M. Porcu, F. Giron, M. Enenkl, T. Kemp, N. Takahashi and Y. Mitsufuji. "Improving Music Source Separation based on Deep Neural Networks through Data Augmentation and Network Blending." ICASSP, 2017
  - A. A. Nugraha, A. Liutkus, and E. Vincent. "Multichannel music separation with deep neural networks." EUSIPCO, 2016.
description:  >
  In our second approach, we use a (stereo) recurrent neural network architecture with bi-directional LSTM (BLSTM) layers. It is trained solemnly on SiSEC DEV and uses data augmentation to avoid overfitting problems. In particular, we use the following augmentations (done on-the-fly to prepare one mini-batch):
  - random swapping left/right channel for each instrument,
  - random scaling with amplitudes from [0.25,1.25],
  - random chunking into sequences for each instrument, and,
  - random combination of instruments from different mixtures
  Finally, the four LSTM outputs are combined by a multi-channel Wiener filter. We estimate the PSDs and spatial covariance matrices for each instrument as described by Nugraha ("weighted scheme").
